[中科院计算技术研究所王晋东：迁移学习的发展和现状](http://app.myzaker.com/news/article.php?pk=5a0ace071bc8e0557c000000)

[Understanding objective functions in neural networks](https://towardsdatascience.com/understanding-objective-functions-in-neural-networks-d217cb068138)

[Uncertainty in deep learning](   http://www.cs.ox.ac.uk/people/yarin.gal/website/blog_2248.html)

[KGAN: How to Break The Minimax Game in GAN](https://arxiv.org/pdf/1711.01744.pdf)

[FEW-SHOT LEARNING WITH GRAPH NEURAL NETWORKS](   https://arxiv.org/pdf/1711.04043.pdf)

[Self-Supervised Intrinsic Image Decomposition](   https://arxiv.org/pdf/1711.03678.pdf)

[Learning without Forgetting](    https://arxiv.org/pdf/1606.09282.pdf)

[backprop may be the answer](  https://medium.com/@oaklandthinktank/actually-backprop-may-be-the-answer-142399e044c5)

[how does the brain learn so much so quickly](  https://www.youtube.com/watch?v=cWzi38-vDbE)

[Information Theory of Deep Learning. Naftali Tishby]( https://www.youtube.com/watch?v=bLqJHjXihK8&feature=youtu.be)

[Top-down Neural Attention by Excitation Backprop]( http://cs-people.bu.edu/jmzhang/EB/ExcitationBackprop.pdf)

[Look and Think Twice: Capturing Top-Down Visual Attention with Feedback Convolutional Neural Networks](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Cao_Look_and_Think_ICCV_2015_paper.pdf)

[Steps Towards Understanding Deep Learning: The Information Bottleneck Connection: ](https://weberna.github.io/jekyll/update/2017/11/08/Information-Bottleneck-Part1.html)

[how could machines learn as efficiency as animals and humans](https://www.bilibili.com/video/av16381827/?from=search&seid=8416449361838452274)

[Advance in Variation Inference](https://128.84.21.199/abs/1711.05597)

[Deep Inception-Residual Laplacian Pyramid Networks for Accurate Single Image Super-Resolution](https://scirate.com/arxiv/1711.05431)

[Learning Explanatory Rules from Noisy Data](https://arxiv.org/abs/1711.04574)


